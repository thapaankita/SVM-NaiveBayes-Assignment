{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SVM & Naïve Bayes Assignment"
      ],
      "metadata": {
        "id": "yR-CS_h4yEnC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q1. What is a Support Vector Machine (SVM), and how does it work?\n",
        "\n",
        "**Answer:**  \n",
        "A Support Vector Machine (SVM) is a supervised machine learning algorithm used for classification and regression tasks.  \n",
        "It works by finding the **optimal hyperplane** that best separates data points of different classes.  \n",
        "\n",
        "- The hyperplane is chosen to **maximize the margin**, i.e., the distance between the hyperplane and the nearest data points from each class (called **support vectors**).  \n",
        "- For **linearly separable data**, SVM finds a straight-line (or hyperplane) separator.  \n",
        "- For **non-linear data**, SVM uses the **kernel trick** to map data into a higher-dimensional space where it can find a linear separator.  \n",
        "- This approach makes SVM powerful for handling both linear and non-linear classification problems.\n",
        "\n",
        "## Q2. Explain the difference between Hard Margin and Soft Margin SVM.\n",
        "\n",
        "**Answer:**  \n",
        "Support Vector Machines can separate data using two main approaches: **Hard Margin** and **Soft Margin**.  \n",
        "\n",
        "- **Hard Margin SVM**  \n",
        "  - Assumes the data is perfectly linearly separable.  \n",
        "  - No misclassification is allowed.  \n",
        "  - Finds the hyperplane with the maximum margin that correctly classifies all points.  \n",
        "  - Very sensitive to noise and outliers.  \n",
        "\n",
        "- **Soft Margin SVM**  \n",
        "  - Allows some misclassifications using slack variables.  \n",
        "  - Balances between maximizing the margin and minimizing classification errors.  \n",
        "  - Controlled by the regularization parameter **C**.  \n",
        "  - More robust and widely used in real-world problems where data is noisy or overlapping.\n",
        "\n",
        "## Q3. What is the Kernel Trick in SVM? Give one example of a kernel and explain its use case.\n",
        "\n",
        "**Answer:**  \n",
        "The **Kernel Trick** is a technique in SVM that allows the algorithm to handle non-linear data by mapping it into a higher-dimensional space, without explicitly computing the transformation.  \n",
        "Instead of working with transformed features, SVM uses a **kernel function** to compute dot products directly in the higher-dimensional space, making it efficient and powerful.  \n",
        "\n",
        "- **Example: Radial Basis Function (RBF) Kernel**  \n",
        "  - Formula: \\( K(x, x') = \\exp(-\\gamma \\|x - x'\\|^2) \\)  \n",
        "  - Use case: When data has complex, non-linear decision boundaries (e.g., classifying images, medical data, or text).  \n",
        "  - RBF can capture local patterns and flexible boundaries, making it one of the most commonly used kernels in practice.\n",
        "\n",
        "## Q4. What is a Naïve Bayes Classifier, and why is it called “naïve”?\n",
        "\n",
        "**Answer:**  \n",
        "A **Naïve Bayes Classifier** is a probabilistic machine learning model based on **Bayes’ Theorem**.  \n",
        "It predicts the probability of a class given input features and chooses the class with the highest probability.  \n",
        "\n",
        "- Formula:  \n",
        "  \\( P(y \\mid X) = \\frac{P(X \\mid y) \\cdot P(y)}{P(X)} \\)  \n",
        "\n",
        "- Why it is called **“naïve”**:  \n",
        "  - It assumes that all features are **conditionally independent** given the class label.  \n",
        "  - In reality, features are often correlated, so this assumption is rarely true.  \n",
        "  - Despite this simplification, it works very well in many applications, especially in **text classification** and **spam filtering**.\n",
        "\n",
        "## Q5. Describe the Gaussian, Multinomial, and Bernoulli Naïve Bayes variants. When would you use each one?\n",
        "\n",
        "**Answer:**  \n",
        "Naïve Bayes has different variants depending on the type of feature distribution:  \n",
        "\n",
        "- **Gaussian Naïve Bayes**  \n",
        "  - Assumes features follow a **normal (Gaussian) distribution**.  \n",
        "  - Suitable for **continuous data** (e.g., medical measurements, sensor values).  \n",
        "\n",
        "- **Multinomial Naïve Bayes**  \n",
        "  - Works with **discrete counts** (e.g., word frequencies).  \n",
        "  - Commonly used in **text classification** with term frequency or TF-IDF features.  \n",
        "\n",
        "- **Bernoulli Naïve Bayes**  \n",
        "  - Assumes **binary features** (presence/absence).  \n",
        "  - Useful when only the occurrence of a feature matters (e.g., whether a word appears in a document, yes/no).  \n",
        "\n",
        "**Summary:**  \n",
        "- Use **Gaussian NB** for continuous data.  \n",
        "- Use **Multinomial NB** for count-based text data.  \n",
        "- Use **Bernoulli NB** for binary feature data.\n",
        "\n",
        "# Q6. Write a Python program to:\n",
        "- Load the Iris dataset  \n",
        "- Train an SVM Classifier with a linear kernel  \n",
        "- Print the model's accuracy and support vectors.  \n",
        "\n",
        "**Answer:**\n",
        "\n"
      ],
      "metadata": {
        "id": "1x6lNdK-FWif"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Iris dataset\n",
        "iris = datasets.load_iris()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    iris.data, iris.target, test_size=0.25, random_state=42, stratify=iris.target\n",
        ")\n",
        "\n",
        "# Train SVM with linear kernel\n",
        "svm_linear = SVC(kernel=\"linear\", random_state=42)\n",
        "svm_linear.fit(X_train, y_train)\n",
        "\n",
        "# Predictions and accuracy\n",
        "y_pred = svm_linear.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Accuracy:\", accuracy)\n",
        "print(\"Number of support vectors per class:\", svm_linear.n_support_)\n",
        "print(\"First 5 support vectors:\\n\", svm_linear.support_vectors_[:5])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ja5TH03N0h2V",
        "outputId": "4eacc93a-af59-4570-f1d1-6b74832c5eea"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 1.0\n",
            "Number of support vectors per class: [ 3 10  9]\n",
            "First 5 support vectors:\n",
            " [[5.1 3.8 1.9 0.4]\n",
            " [4.8 3.4 1.9 0.2]\n",
            " [5.  3.  1.6 0.2]\n",
            " [6.9 3.1 4.9 1.5]\n",
            " [6.  2.9 4.5 1.5]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q7. Write a Python program to:\n",
        "- Load the Breast Cancer dataset  \n",
        "- Train a Gaussian Naïve Bayes model  \n",
        "- Print its classification report including precision, recall, and F1-score.  \n",
        "\n",
        "**Answer (using Breast Cancer dataset):**"
      ],
      "metadata": {
        "id": "MCPG1hWTGMyz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import GaussianNB\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Load Breast Cancer dataset\n",
        "cancer = datasets.load_breast_cancer()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    cancer.data, cancer.target, test_size=0.25, random_state=42, stratify=cancer.target\n",
        ")\n",
        "\n",
        "# Train Gaussian Naive Bayes\n",
        "gnb = GaussianNB()\n",
        "gnb.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = gnb.predict(X_test)\n",
        "\n",
        "# Print classification report\n",
        "print(\"Classification Report:\\n\")\n",
        "print(classification_report(y_test, y_pred))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xt9HqagE1sEb",
        "outputId": "4603d3d9-167a-40af-fb54-258ee78b81cb"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.96      0.87      0.91        53\n",
            "           1       0.93      0.98      0.95        90\n",
            "\n",
            "    accuracy                           0.94       143\n",
            "   macro avg       0.94      0.92      0.93       143\n",
            "weighted avg       0.94      0.94      0.94       143\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q8. Write a Python program to:\n",
        "- Train an SVM Classifier on the Wine dataset using GridSearchCV to find the best C and gamma.  \n",
        "- Print the best hyperparameters and accuracy.  \n",
        "\n",
        "**Answer (using Wine dataset):**\n"
      ],
      "metadata": {
        "id": "nEQAQ0JNGhHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Load Wine dataset\n",
        "wine = datasets.load_wine()\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    wine.data, wine.target, test_size=0.25, random_state=42, stratify=wine.target\n",
        ")\n",
        "\n",
        "# Define parameter grid for GridSearchCV\n",
        "param_grid = {\n",
        "    \"C\": [0.1, 1, 10, 100],\n",
        "    \"gamma\": [0.001, 0.01, 0.1, 1],\n",
        "    \"kernel\": [\"rbf\"]\n",
        "}\n",
        "\n",
        "# Train SVM with GridSearchCV\n",
        "grid = GridSearchCV(SVC(), param_grid, cv=5, scoring=\"accuracy\")\n",
        "grid.fit(X_train, y_train)\n",
        "\n",
        "# Best model\n",
        "best_model = grid.best_estimator_\n",
        "\n",
        "# Predictions and accuracy\n",
        "y_pred = best_model.predict(X_test)\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "\n",
        "# Print results\n",
        "print(\"Best Parameters:\", grid.best_params_)\n",
        "print(\"Accuracy on Test Set:\", accuracy)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t-lySktw1De8",
        "outputId": "6cfe9c05-c8f6-4c76-9d47-c86f9421b19b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Best Parameters: {'C': 100, 'gamma': 0.001, 'kernel': 'rbf'}\n",
            "Accuracy on Test Set: 0.7777777777777778\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q9. Write a Python program to:  \n",
        "● Train a Naïve Bayes Classifier on a synthetic text dataset (e.g. using sklearn.datasets.fetch_20newsgroups).  \n",
        "● Print the model's ROC-AUC score for its predictions.  \n",
        "\n",
        "**Answer:**"
      ],
      "metadata": {
        "id": "B3Tkr5w8GrN9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import label_binarize\n",
        "\n",
        "# Load synthetic text dataset (20 newsgroups)\n",
        "newsgroups = fetch_20newsgroups(subset=\"all\", categories=['rec.sport.baseball', 'sci.med'], shuffle=True, random_state=42)\n",
        "\n",
        "X, y = newsgroups.data, newsgroups.target\n",
        "\n",
        "# Split dataset\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
        "\n",
        "# Text vectorization using TF-IDF\n",
        "vectorizer = TfidfVectorizer(stop_words='english', max_features=5000)\n",
        "X_train_vec = vectorizer.fit_transform(X_train)\n",
        "X_test_vec = vectorizer.transform(X_test)\n",
        "\n",
        "# Train Naive Bayes Classifier\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train_vec, y_train)\n",
        "\n",
        "# Predict probabilities\n",
        "y_prob = nb.predict_proba(X_test_vec)[:, 1]\n",
        "\n",
        "# ROC-AUC score\n",
        "roc_auc = roc_auc_score(y_test, y_prob)\n",
        "\n",
        "print(\"ROC-AUC Score:\", roc_auc)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9KQ7_S1v2EyC",
        "outputId": "5a605163-0ce3-4e32-febe-617a6ae3b309"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ROC-AUC Score: 0.9999187031526917\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Q10. Imagine you’re working as a data scientist for a company that handles email communications.  \n",
        "Your task is to automatically classify emails as Spam or Not Spam. The emails may contain:  \n",
        "● Text with diverse vocabulary  \n",
        "● Potential class imbalance (far more legitimate emails than spam)  \n",
        "● Some incomplete or missing data  \n",
        "\n",
        "Explain the approach you would take to:  \n",
        "● Preprocess the data (e.g. text vectorization, handling missing data)  \n",
        "● Choose and justify an appropriate model (SVM vs. Naïve Bayes)  \n",
        "● Address class imbalance  \n",
        "● Evaluate the performance of your solution with suitable metrics  \n",
        "And explain the business impact of your solution.  \n",
        "\n",
        "**Answer:**  \n",
        "### 1. Data Preprocessing\n",
        "- **Handling Missing Data:** Replace missing values with empty strings or use imputation methods.  \n",
        "- **Text Vectorization:** Use **TF-IDF Vectorizer** or **CountVectorizer** to convert raw text into numerical features.  \n",
        "- **Feature Scaling:** Not required for Naïve Bayes, but useful for SVM.  \n",
        "\n",
        "### 2. Model Choice  \n",
        "- **Naïve Bayes (Preferred):** Works well for text data due to the independence assumption and efficiency in high-dimensional sparse data.  \n",
        "- **SVM:** Can also perform well, but may be computationally more expensive on very large text datasets.  \n",
        " For spam classification, **Multinomial Naïve Bayes** is typically the most effective and efficient.  \n",
        "\n",
        "### 3. Handling Class Imbalance  \n",
        "- Use **SMOTE (Synthetic Minority Oversampling Technique)** or **Random Oversampling** to balance the dataset.  \n",
        "- Alternatively, apply **class weights** in the model to penalize misclassification of the minority class (spam).  \n",
        "\n",
        "### 4. Evaluation Metrics  \n",
        "- **Accuracy alone is not enough** due to class imbalance.  \n",
        "- Use **Precision, Recall, and F1-score** to evaluate spam detection.  \n",
        "- **ROC-AUC score** can measure overall model performance.  \n",
        "- High **recall** is important to catch as much spam as possible, while maintaining a good **precision** to avoid flagging legitimate emails.  \n",
        "\n",
        "### 5. Business Impact  \n",
        "- **Improved productivity:** Employees waste less time deleting spam manually.  \n",
        "- **Better security:** Reduces risk of phishing and malware from spam emails.  \n",
        "- **Customer trust:** Ensures important legitimate emails are not wrongly classified as spam.  \n",
        "- **Cost savings:** Automating spam detection reduces reliance on manual checks and IT interventions.  \n",
        "\n",
        "---\n",
        "### Example Python Code:"
      ],
      "metadata": {
        "id": "jMKNDE0EG8r9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import fetch_20newsgroups\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "from sklearn.metrics import classification_report, roc_auc_score\n",
        "from imblearn.over_sampling import RandomOverSampler\n",
        "\n",
        "# Load dataset (simulating spam vs not spam using 2 categories)\n",
        "data = fetch_20newsgroups(subset=\"all\",\n",
        "                          categories=[\"sci.space\", \"rec.autos\"],\n",
        "                          shuffle=True, random_state=42)\n",
        "\n",
        "X, y = data.data, data.target\n",
        "\n",
        "# Handle missing data (replace None with empty string)\n",
        "X = [text if text is not None else \"\" for text in X]\n",
        "\n",
        "# Convert text into numerical features (TF-IDF)\n",
        "vectorizer = TfidfVectorizer(stop_words=\"english\", max_features=5000)\n",
        "X_vec = vectorizer.fit_transform(X)\n",
        "\n",
        "# Handle class imbalance (oversampling minority class)\n",
        "ros = RandomOverSampler(random_state=42)\n",
        "X_resampled, y_resampled = ros.fit_resample(X_vec, y)\n",
        "\n",
        "# Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X_resampled, y_resampled, test_size=0.25, random_state=42\n",
        ")\n",
        "\n",
        "# Train Naïve Bayes model\n",
        "nb = MultinomialNB()\n",
        "nb.fit(X_train, y_train)\n",
        "\n",
        "# Predictions\n",
        "y_pred = nb.predict(X_test)\n",
        "y_prob = nb.predict_proba(X_test)[:, 1]\n",
        "\n",
        "# Evaluation\n",
        "print(\"Classification Report:\\n\", classification_report(y_test, y_pred))\n",
        "print(\"ROC-AUC Score:\", roc_auc_score(y_test, y_prob))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jve5JIaO2Yle",
        "outputId": "1f52115d-3ec4-433f-e4db-b0ee0710fc13"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "               precision    recall  f1-score   support\n",
            "\n",
            "           0       0.99      1.00      0.99       252\n",
            "           1       1.00      0.99      0.99       243\n",
            "\n",
            "    accuracy                           0.99       495\n",
            "   macro avg       0.99      0.99      0.99       495\n",
            "weighted avg       0.99      0.99      0.99       495\n",
            "\n",
            "ROC-AUC Score: 0.9998203671043177\n"
          ]
        }
      ]
    }
  ]
}